{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Enhanced Fuel Consumption Prediction Model\n",
    "## Complete Machine Learning Pipeline with Model Comparison and PKL Output\n",
    "\n",
    "This notebook provides a comprehensive analysis of fuel consumption data from 2000-2022, including:\n",
    "- Data exploration and visualization\n",
    "- Multiple model training and comparison\n",
    "- Best model selection and PKL file output\n",
    "- Feature importance analysis\n",
    "- Enhanced prediction capabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 2. Configuration and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_PATH = 'Fuel_Consumption_2000-2022.csv'\n",
    "MODEL_OUTPUT_DIR = 'model'\n",
    "\n",
    "# Define input and output features\n",
    "INPUT_FEATURES = [\n",
    "    'MAKE', 'MODEL', 'VEHICLE CLASS', \n",
    "    'ENGINE SIZE', 'CYLINDERS', 'TRANSMISSION', 'FUEL'\n",
    "]\n",
    "\n",
    "OUTPUT_FEATURES = ['COMB (L/100 km)', 'HWY (L/100 km)', 'EMISSIONS']\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Input features: {INPUT_FEATURES}\")\n",
    "print(f\"Output features: {OUTPUT_FEATURES}\")\n",
    "print(f\"Model output directory: {MODEL_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    \"\"\"Load and validate data from CSV file.\"\"\"\n",
    "    print(f\"Loading data from {filepath}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"✓ Data loaded successfully. Shape: {df.shape}\")\n",
    "        print(f\"✓ Columns available: {list(df.columns)}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the dataset\n",
    "df = load_data(DATA_PATH)\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"\\nDataset overview:\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exploration-header",
   "metadata": {},
   "source": [
    "## 3. Data Exploration and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_basic_info(df):\n",
    "    \"\"\"Display basic information about the dataset.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"BASIC DATASET INFORMATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"Dataset Shape: {df.shape}\")\n",
    "    print(f\"Memory Usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    print(\"\\nData Types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"\\nMissing Values:\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing,\n",
    "        'Missing Percentage': missing_pct\n",
    "    })[missing > 0]\n",
    "    \n",
    "    if len(missing_df) > 0:\n",
    "        print(missing_df)\n",
    "    else:\n",
    "        print(\"No missing values found!\")\n",
    "    \n",
    "    print(\"\\nBasic Statistics for Numerical Columns:\")\n",
    "    print(df.describe())\n",
    "\n",
    "if df is not None:\n",
    "    explore_basic_info(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-visual",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_visualizations(df):\n",
    "    \"\"\"Create comprehensive data visualizations.\"\"\"\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Distribution of fuel consumption\n",
    "    plt.subplot(3, 3, 1)\n",
    "    plt.hist(df['COMB (L/100 km)'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.title('Distribution of Combined Fuel Consumption')\n",
    "    plt.xlabel('L/100 km')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Distribution of highway fuel consumption\n",
    "    plt.subplot(3, 3, 2)\n",
    "    plt.hist(df['HWY (L/100 km)'], bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    plt.title('Distribution of Highway Fuel Consumption')\n",
    "    plt.xlabel('L/100 km')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Distribution of emissions\n",
    "    plt.subplot(3, 3, 3)\n",
    "    plt.hist(df['EMISSIONS'], bins=50, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    plt.title('Distribution of CO2 Emissions')\n",
    "    plt.xlabel('g/km')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Fuel type distribution\n",
    "    plt.subplot(3, 3, 4)\n",
    "    fuel_counts = df['FUEL'].value_counts()\n",
    "    plt.pie(fuel_counts.values, labels=fuel_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    plt.title('Fuel Type Distribution')\n",
    "    \n",
    "    # 5. Vehicle class distribution (top 10)\n",
    "    plt.subplot(3, 3, 5)\n",
    "    vc_counts = df['VEHICLE CLASS'].value_counts().head(10)\n",
    "    plt.barh(range(len(vc_counts)), vc_counts.values)\n",
    "    plt.yticks(range(len(vc_counts)), vc_counts.index)\n",
    "    plt.title('Top 10 Vehicle Classes')\n",
    "    plt.xlabel('Count')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Engine size vs Combined consumption scatter\n",
    "    plt.subplot(3, 3, 6)\n",
    "    plt.scatter(df['ENGINE SIZE'], df['COMB (L/100 km)'], alpha=0.5, s=1)\n",
    "    plt.xlabel('Engine Size (L)')\n",
    "    plt.ylabel('Combined Consumption (L/100 km)')\n",
    "    plt.title('Engine Size vs Fuel Consumption')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 7. Cylinders vs Combined consumption box plot\n",
    "    plt.subplot(3, 3, 7)\n",
    "    df_sample = df.sample(n=min(10000, len(df)))  # Sample for performance\n",
    "    sns.boxplot(data=df_sample, x='CYLINDERS', y='COMB (L/100 km)')\n",
    "    plt.title('Cylinders vs Fuel Consumption')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 8. Year trend\n",
    "    plt.subplot(3, 3, 8)\n",
    "    yearly_avg = df.groupby('YEAR')['COMB (L/100 km)'].mean()\n",
    "    plt.plot(yearly_avg.index, yearly_avg.values, marker='o', linewidth=2, markersize=6)\n",
    "    plt.title('Average Fuel Consumption Trend Over Years')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Average Combined Consumption (L/100 km)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 9. Make distribution (top 10)\n",
    "    plt.subplot(3, 3, 9)\n",
    "    make_counts = df['MAKE'].value_counts().head(10)\n",
    "    plt.barh(range(len(make_counts)), make_counts.values)\n",
    "    plt.yticks(range(len(make_counts)), make_counts.index)\n",
    "    plt.title('Top 10 Vehicle Makes')\n",
    "    plt.xlabel('Count')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if df is not None:\n",
    "    create_data_visualizations(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correlation-header",
   "metadata": {},
   "source": [
    "## 4. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_correlations(df):\n",
    "    \"\"\"Analyze correlations between numerical variables.\"\"\"\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    correlation_matrix = df[numerical_cols].corr()\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(correlation_matrix, \n",
    "                annot=True, \n",
    "                cmap='RdBu_r', \n",
    "                center=0, \n",
    "                square=True,\n",
    "                fmt='.3f',\n",
    "                cbar_kws={'shrink': 0.8})\n",
    "    plt.title('Correlation Matrix of Numerical Features', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show strongest correlations with target variables\n",
    "    target_correlations = correlation_matrix[OUTPUT_FEATURES].drop(OUTPUT_FEATURES)\n",
    "    print(\"\\nStrongest correlations with target variables:\")\n",
    "    for target in OUTPUT_FEATURES:\n",
    "        print(f\"\\n{target}:\")\n",
    "        corr_sorted = target_correlations[target].abs().sort_values(ascending=False)\n",
    "        for feature, corr in corr_sorted.head(5).items():\n",
    "            print(f\"  {feature}: {target_correlations[target][feature]:.3f}\")\n",
    "\n",
    "if df is not None:\n",
    "    analyze_correlations(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing-header",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, input_features, output_features):\n",
    "    \"\"\"Comprehensive data preprocessing pipeline.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATA PREPROCESSING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Select relevant columns and create copy\n",
    "    all_features = input_features + output_features\n",
    "    data = df[all_features].copy()\n",
    "    \n",
    "    print(f\"Initial data shape: {data.shape}\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    missing_before = data.isnull().sum().sum()\n",
    "    if missing_before > 0:\n",
    "        print(f\"Missing values found: {missing_before}\")\n",
    "        data_clean = data.dropna()\n",
    "        print(f\"After removing missing values: {data_clean.shape}\")\n",
    "    else:\n",
    "        data_clean = data\n",
    "        print(\"No missing values to handle\")\n",
    "    \n",
    "    # Separate features and targets\n",
    "    X = data_clean[input_features]\n",
    "    y = data_clean[output_features]\n",
    "    \n",
    "    # Identify feature types\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_features = X.select_dtypes(include=['number']).columns.tolist()\n",
    "    \n",
    "    print(f\"\\nFeature analysis:\")\n",
    "    print(f\"  Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "    print(f\"  Numerical features ({len(numerical_features)}): {numerical_features}\")\n",
    "    \n",
    "    # Create preprocessing pipeline\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'  # Keep numerical features as is\n",
    "    )\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=None\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nData split:\")\n",
    "    print(f\"  Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"  Testing set: {X_test.shape[0]} samples\")\n",
    "    print(f\"  Split ratio: {X_train.shape[0]/(X_train.shape[0]+X_test.shape[0]):.1%} train, {X_test.shape[0]/(X_train.shape[0]+X_test.shape[0]):.1%} test\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, preprocessor, categorical_features, numerical_features\n",
    "\n",
    "# Preprocess the data\n",
    "if df is not None:\n",
    "    X_train, X_test, y_train, y_test, preprocessor, categorical_features, numerical_features = preprocess_data(\n",
    "        df, INPUT_FEATURES, OUTPUT_FEATURES\n",
    "    )\n",
    "    \n",
    "    print(\"\\nPreprocessing completed successfully!\")\n",
    "    print(f\"Ready to train models with {X_train.shape[1]} input features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models-header",
   "metadata": {},
   "source": [
    "## 6. Model Training and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-definitions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_models():\n",
    "    \"\"\"Define multiple models for comparison.\"\"\"\n",
    "    models = {\n",
    "        'Random Forest': MultiOutputRegressor(RandomForestRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=15,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )),\n",
    "        \n",
    "        'Gradient Boosting': MultiOutputRegressor(GradientBoostingRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42\n",
    "        )),\n",
    "        \n",
    "        'Decision Tree': MultiOutputRegressor(DecisionTreeRegressor(\n",
    "            max_depth=15,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=4,\n",
    "            random_state=42\n",
    "        )),\n",
    "        \n",
    "        'Linear Regression': MultiOutputRegressor(LinearRegression()),\n",
    "        \n",
    "        'Ridge Regression': MultiOutputRegressor(Ridge(\n",
    "            alpha=1.0,\n",
    "            random_state=42\n",
    "        )),\n",
    "        \n",
    "        'Neural Network': MultiOutputRegressor(MLPRegressor(\n",
    "            hidden_layer_sizes=(100, 50, 25),\n",
    "            max_iter=500,\n",
    "            random_state=42,\n",
    "            early_stopping=True,\n",
    "            validation_fraction=0.1,\n",
    "            alpha=0.001\n",
    "        ))\n",
    "    }\n",
    "    \n",
    "    return models\n",
    "\n",
    "models = define_models()\n",
    "print(f\"Defined {len(models)} models for comparison:\")\n",
    "for name in models.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(models, X_train, X_test, y_train, y_test, preprocessor, output_features):\n",
    "    \"\"\"Train and evaluate multiple models.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MODEL TRAINING AND EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    trained_models = {}\n",
    "    all_metrics = {}\n",
    "    training_times = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n🔄 Training {name}...\")\n",
    "        \n",
    "        # Create pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            # Train model with timing\n",
    "            import time\n",
    "            start_time = time.time()\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            training_time = time.time() - start_time\n",
    "            training_times[name] = training_time\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics for each target\n",
    "            model_metrics = {}\n",
    "            total_r2, total_rmse, total_mae = 0, 0, 0\n",
    "            \n",
    "            for i, target in enumerate(output_features):\n",
    "                y_true = y_test[target]\n",
    "                y_pred_target = y_pred[:, i] if y_pred.ndim > 1 else y_pred\n",
    "                \n",
    "                rmse = np.sqrt(mean_squared_error(y_true, y_pred_target))\n",
    "                mae = mean_absolute_error(y_true, y_pred_target)\n",
    "                r2 = r2_score(y_true, y_pred_target)\n",
    "                \n",
    "                model_metrics[target] = {\n",
    "                    'rmse': rmse,\n",
    "                    'mae': mae,\n",
    "                    'r2': r2\n",
    "                }\n",
    "                \n",
    "                total_r2 += r2\n",
    "                total_rmse += rmse\n",
    "                total_mae += mae\n",
    "            \n",
    "            # Calculate averages\n",
    "            n_targets = len(output_features)\n",
    "            model_metrics['average'] = {\n",
    "                'r2': total_r2 / n_targets,\n",
    "                'rmse': total_rmse / n_targets,\n",
    "                'mae': total_mae / n_targets\n",
    "            }\n",
    "            \n",
    "            trained_models[name] = pipeline\n",
    "            all_metrics[name] = model_metrics\n",
    "            \n",
    "            # Display results\n",
    "            avg_r2 = model_metrics['average']['r2']\n",
    "            avg_rmse = model_metrics['average']['rmse']\n",
    "            print(f\"✓ {name} completed in {training_time:.2f}s\")\n",
    "            print(f\"  Average R²: {avg_r2:.4f}\")\n",
    "            print(f\"  Average RMSE: {avg_rmse:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error training {name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return trained_models, all_metrics, training_times\n",
    "\n",
    "# Train all models\n",
    "if 'X_train' in locals():\n",
    "    trained_models, all_metrics, training_times = train_and_evaluate_models(\n",
    "        models, X_train, X_test, y_train, y_test, preprocessor, OUTPUT_FEATURES\n",
    "    )\n",
    "    print(\"\\n🎉 Model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "## 7. Model Comparison and Best Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_and_visualize_models(all_metrics, training_times):\n",
    "    \"\"\"Compare models and visualize performance.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MODEL COMPARISON RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = []\n",
    "    for name, metrics in all_metrics.items():\n",
    "        comparison_data.append({\n",
    "            'Model': name,\n",
    "            'Avg_R2': metrics['average']['r2'],\n",
    "            'Avg_RMSE': metrics['average']['rmse'],\n",
    "            'Avg_MAE': metrics['average']['mae'],\n",
    "            'Training_Time': training_times.get(name, 0)\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.sort_values('Avg_R2', ascending=False)\n",
    "    \n",
    "    print(\"\\nModel Performance Summary:\")\n",
    "    print(comparison_df.round(4))\n",
    "    \n",
    "    # Find best model\n",
    "    best_model_name = comparison_df.iloc[0]['Model']\n",
    "    best_r2 = comparison_df.iloc[0]['Avg_R2']\n",
    "    best_rmse = comparison_df.iloc[0]['Avg_RMSE']\n",
    "    \n",
    "    print(f\"\\n🏆 Best Performing Model: {best_model_name}\")\n",
    "    print(f\"   Average R²: {best_r2:.4f}\")\n",
    "    print(f\"   Average RMSE: {best_rmse:.4f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # R² Score comparison\n",
    "    bars1 = ax1.bar(comparison_df['Model'], comparison_df['Avg_R2'], \n",
    "                    color=['gold' if name == best_model_name else 'lightblue' \n",
    "                          for name in comparison_df['Model']])\n",
    "    ax1.set_title('Model Comparison - R² Score', fontweight='bold')\n",
    "    ax1.set_ylabel('R² Score')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars1, comparison_df['Avg_R2']):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # RMSE comparison\n",
    "    bars2 = ax2.bar(comparison_df['Model'], comparison_df['Avg_RMSE'],\n",
    "                    color=['gold' if name == best_model_name else 'lightcoral' \n",
    "                          for name in comparison_df['Model']])\n",
    "    ax2.set_title('Model Comparison - RMSE', fontweight='bold')\n",
    "    ax2.set_ylabel('RMSE')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars2, comparison_df['Avg_RMSE']):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(comparison_df['Avg_RMSE'])*0.02, \n",
    "                f'{value:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Training time comparison\n",
    "    bars3 = ax3.bar(comparison_df['Model'], comparison_df['Training_Time'],\n",
    "                    color='lightgreen')\n",
    "    ax3.set_title('Training Time Comparison', fontweight='bold')\n",
    "    ax3.set_ylabel('Time (seconds)')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Performance vs Time scatter\n",
    "    scatter = ax4.scatter(comparison_df['Training_Time'], comparison_df['Avg_R2'], \n",
    "                         s=100, alpha=0.7, c=['gold' if name == best_model_name else 'blue' \n",
    "                                              for name in comparison_df['Model']])\n",
    "    ax4.set_xlabel('Training Time (seconds)')\n",
    "    ax4.set_ylabel('R² Score')\n",
    "    ax4.set_title('Performance vs Training Time', fontweight='bold')\n",
    "    \n",
    "    # Add model name labels\n",
    "    for i, name in enumerate(comparison_df['Model']):\n",
    "        ax4.annotate(name, (comparison_df.iloc[i]['Training_Time'], comparison_df.iloc[i]['Avg_R2']),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return best_model_name, comparison_df\n",
    "\n",
    "if 'all_metrics' in locals():\n",
    "    best_model_name, comparison_df = compare_and_visualize_models(all_metrics, training_times)\n",
    "    best_model = trained_models[best_model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-evaluation-header",
   "metadata": {},
   "source": [
    "## 8. Detailed Evaluation of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_model_evaluation(model, X_test, y_test, model_name, output_features):\n",
    "    \"\"\"Perform detailed evaluation of the best model.\"\"\"\n",
    "    print(f\"=\" * 60)\n",
    "    print(f\"DETAILED EVALUATION - {model_name}\")\n",
    "    print(f\"=\" * 60)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Create detailed visualizations\n",
    "    fig, axes = plt.subplots(2, len(output_features), figsize=(18, 12))\n",
    "    if len(output_features) == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    detailed_metrics = {}\n",
    "    \n",
    "    for i, target in enumerate(output_features):\n",
    "        y_true = y_test[target]\n",
    "        y_pred_target = y_pred[:, i] if y_pred.ndim > 1 else y_pred\n",
    "        \n",
    "        # Calculate metrics\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred_target))\n",
    "        mae = mean_absolute_error(y_true, y_pred_target)\n",
    "        r2 = r2_score(y_true, y_pred_target)\n",
    "        \n",
    "        # Calculate MAPE (Mean Absolute Percentage Error)\n",
    "        mape = np.mean(np.abs((y_true - y_pred_target) / y_true)) * 100\n",
    "        \n",
    "        detailed_metrics[target] = {\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R²': r2,\n",
    "            'MAPE': mape\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{target} Metrics:\")\n",
    "        print(f\"  RMSE: {rmse:.4f}\")\n",
    "        print(f\"  MAE: {mae:.4f}\")\n",
    "        print(f\"  R²: {r2:.4f}\")\n",
    "        print(f\"  MAPE: {mape:.2f}%\")\n",
    "        \n",
    "        # Actual vs Predicted scatter plot\n",
    "        axes[0, i].scatter(y_true, y_pred_target, alpha=0.5, s=1)\n",
    "        axes[0, i].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], \n",
    "                       'r--', lw=2, label='Perfect Prediction')\n",
    "        axes[0, i].set_xlabel(f'Actual {target}')\n",
    "        axes[0, i].set_ylabel(f'Predicted {target}')\n",
    "        axes[0, i].set_title(f'{target}\\nR² = {r2:.4f}')\n",
    "        axes[0, i].legend()\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Residual plot\n",
    "        residuals = y_true - y_pred_target\n",
    "        axes[1, i].scatter(y_pred_target, residuals, alpha=0.5, s=1)\n",
    "        axes[1, i].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "        axes[1, i].set_xlabel(f'Predicted {target}')\n",
    "        axes[1, i].set_ylabel('Residuals')\n",
    "        axes[1, i].set_title(f'Residual Plot - {target}')\n",
    "        axes[1, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return detailed_metrics\n",
    "\n",
    "if 'best_model' in locals():\n",
    "    detailed_metrics = detailed_model_evaluation(\n",
    "        best_model, X_test, y_test, best_model_name, OUTPUT_FEATURES\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-importance-header",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, categorical_features, numerical_features, preprocessor, model_name):\n",
    "    \"\"\"Analyze and visualize feature importance for tree-based models.\"\"\"\n",
    "    print(f\"=\" * 60)\n",
    "    print(f\"FEATURE IMPORTANCE ANALYSIS - {model_name}\")\n",
    "    print(f\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Check if model supports feature importance\n",
    "        if hasattr(model.named_steps['model'], 'estimators_'):\n",
    "            # Get feature names after preprocessing\n",
    "            ohe = preprocessor.named_transformers_['cat']\n",
    "            cat_feature_names = ohe.get_feature_names_out(categorical_features)\n",
    "            all_feature_names = np.concatenate([cat_feature_names, numerical_features])\n",
    "            \n",
    "            # Extract feature importances\n",
    "            estimators = model.named_steps['model'].estimators_\n",
    "            n_features = len(all_feature_names)\n",
    "            total_importances = np.zeros(n_features)\n",
    "            \n",
    "            # Average importance across all estimators\n",
    "            for estimator in estimators:\n",
    "                if hasattr(estimator, 'feature_importances_'):\n",
    "                    total_importances += estimator.feature_importances_\n",
    "            \n",
    "            avg_importances = total_importances / len(estimators)\n",
    "            \n",
    "            # Create importance DataFrame\n",
    "            importance_df = pd.DataFrame({\n",
    "                'Feature': all_feature_names,\n",
    "                'Importance': avg_importances\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            print(f\"\\nTop 20 Most Important Features:\")\n",
    "            top_features = importance_df.head(20)\n",
    "            print(top_features)\n",
    "            \n",
    "            # Visualize feature importance\n",
    "            plt.figure(figsize=(14, 10))\n",
    "            sns.barplot(data=top_features, y='Feature', x='Importance', palette='viridis')\n",
    "            plt.title(f'Top 20 Feature Importance - {model_name}', fontsize=16, fontweight='bold')\n",
    "            plt.xlabel('Importance Score', fontsize=12)\n",
    "            plt.ylabel('Features', fontsize=12)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Group importance by original feature categories\n",
    "            feature_groups = {}\n",
    "            for feature, importance in zip(all_feature_names, avg_importances):\n",
    "                # Determine original feature group\n",
    "                original_feature = None\n",
    "                for cat_feat in categorical_features:\n",
    "                    if feature.startswith(f\"{cat_feat}_\"):\n",
    "                        original_feature = cat_feat\n",
    "                        break\n",
    "                if original_feature is None and feature in numerical_features:\n",
    "                    original_feature = feature\n",
    "                \n",
    "                if original_feature:\n",
    "                    if original_feature not in feature_groups:\n",
    "                        feature_groups[original_feature] = 0\n",
    "                    feature_groups[original_feature] += importance\n",
    "            \n",
    "            # Plot grouped importance\n",
    "            groups_df = pd.DataFrame(\n",
    "                list(feature_groups.items()), \n",
    "                columns=['Feature_Group', 'Total_Importance']\n",
    "            ).sort_values('Total_Importance', ascending=False)\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.barplot(data=groups_df, x='Feature_Group', y='Total_Importance', palette='Set2')\n",
    "            plt.title(f'Feature Group Importance - {model_name}', fontsize=16, fontweight='bold')\n",
    "            plt.xlabel('Feature Groups', fontsize=12)\n",
    "            plt.ylabel('Total Importance Score', fontsize=12)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"\\nFeature Group Importance:\")\n",
    "            print(groups_df)\n",
    "            \n",
    "            return importance_df, groups_df\n",
    "            \n",
    "        else:\n",
    "            print(f\"Feature importance not available for {model_name}\")\n",
    "            return None, None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing feature importance: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "if 'best_model' in locals() and 'categorical_features' in locals():\n",
    "    importance_df, groups_df = analyze_feature_importance(\n",
    "        best_model, categorical_features, numerical_features, preprocessor, best_model_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-model-header",
   "metadata": {},
   "source": [
    "## 10. Save Best Model as PKL File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model_pkl(model, model_name, metrics, output_dir='model'):\n",
    "    \"\"\"Save the best model as a PKL file with metadata.\"\"\"\n",
    "    print(f\"=\" * 60)\n",
    "    print(f\"SAVING BEST MODEL TO PKL FILE\")\n",
    "    print(f\"=\" * 60)\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create model filename\n",
    "    safe_model_name = model_name.lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "    timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_filename = f'best_fuel_model_{safe_model_name}_{timestamp}.pkl'\n",
    "    model_path = os.path.join(output_dir, model_filename)\n",
    "    \n",
    "    # Prepare model package with metadata\n",
    "    model_package = {\n",
    "        'model': model,\n",
    "        'model_name': model_name,\n",
    "        'metrics': metrics,\n",
    "        'input_features': INPUT_FEATURES,\n",
    "        'output_features': OUTPUT_FEATURES,\n",
    "        'timestamp': pd.Timestamp.now(),\n",
    "        'sklearn_version': __import__('sklearn').__version__,\n",
    "        'pandas_version': pd.__version__,\n",
    "        'numpy_version': np.__version__\n",
    "    }\n",
    "    \n",
    "    # Save the model\n",
    "    joblib.dump(model_package, model_path, compress=3)\n",
    "    \n",
    "    # Verify file was created and get size\n",
    "    if os.path.exists(model_path):\n",
    "        file_size = os.path.getsize(model_path) / (1024 * 1024)  # Size in MB\n",
    "        print(f\"✓ Model successfully saved to: {model_path}\")\n",
    "        print(f\"✓ File size: {file_size:.2f} MB\")\n",
    "        print(f\"✓ Model type: {model_name}\")\n",
    "        print(f\"✓ Average R²: {metrics['average']['r2']:.4f}\")\n",
    "        print(f\"✓ Average RMSE: {metrics['average']['rmse']:.4f}\")\n",
    "        \n",
    "        # Create a simple model info file\n",
    "        info_filename = f'model_info_{safe_model_name}_{timestamp}.txt'\n",
    "        info_path = os.path.join(output_dir, info_filename)\n",
    "        \n",
    "        with open(info_path, 'w') as f:\n",
    "            f.write(f\"Fuel Consumption Prediction Model\\n\")\n",
    "            f.write(f\"{'='*40}\\n\\n\")\n",
    "            f.write(f\"Model Type: {model_name}\\n\")\n",
    "            f.write(f\"Created: {pd.Timestamp.now()}\\n\")\n",
    "            f.write(f\"File: {model_filename}\\n\\n\")\n",
    "            \n",
    "            f.write(f\"Performance Metrics:\\n\")\n",
    "            f.write(f\"Average R²: {metrics['average']['r2']:.4f}\\n\")\n",
    "            f.write(f\"Average RMSE: {metrics['average']['rmse']:.4f}\\n\")\n",
    "            f.write(f\"Average MAE: {metrics['average']['mae']:.4f}\\n\\n\")\n",
    "            \n",
    "            f.write(f\"Target Variables:\\n\")\n",
    "            for target in OUTPUT_FEATURES:\n",
    "                if target in metrics:\n",
    "                    f.write(f\"  {target}:\\n\")\n",
    "                    f.write(f\"    R²: {metrics[target]['r2']:.4f}\\n\")\n",
    "                    f.write(f\"    RMSE: {metrics[target]['rmse']:.4f}\\n\")\n",
    "                    f.write(f\"    MAE: {metrics[target]['mae']:.4f}\\n\")\n",
    "            \n",
    "            f.write(f\"\\nInput Features: {INPUT_FEATURES}\\n\")\n",
    "            f.write(f\"Output Features: {OUTPUT_FEATURES}\\n\")\n",
    "        \n",
    "        print(f\"✓ Model info saved to: {info_path}\")\n",
    "        \n",
    "        return model_path, info_path\n",
    "    else:\n",
    "        print(f\"✗ Error: Failed to save model to {model_path}\")\n",
    "        return None, None\n",
    "\n",
    "if 'best_model' in locals() and 'all_metrics' in locals():\n",
    "    model_path, info_path = save_best_model_pkl(\n",
    "        best_model, best_model_name, all_metrics[best_model_name], MODEL_OUTPUT_DIR\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prediction-demo-header",
   "metadata": {},
   "source": [
    "## 11. Enhanced Prediction Function and Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prediction-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prediction_function(model):\n",
    "    \"\"\"Create an enhanced prediction function.\"\"\"\n",
    "    \n",
    "    def predict_fuel_consumption(vehicle_specs):\n",
    "        \"\"\"\n",
    "        Predict fuel consumption and emissions for a vehicle.\n",
    "        \n",
    "        Args:\n",
    "            vehicle_specs (dict): Dictionary with vehicle specifications\n",
    "                Required keys: 'MAKE', 'MODEL', 'VEHICLE CLASS', 'ENGINE SIZE', \n",
    "                              'CYLINDERS', 'TRANSMISSION', 'FUEL'\n",
    "        \n",
    "        Returns:\n",
    "            dict: Comprehensive predictions and analysis\n",
    "        \"\"\"\n",
    "        # Convert to DataFrame\n",
    "        input_df = pd.DataFrame([vehicle_specs])\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = model.predict(input_df)[0]\n",
    "        \n",
    "        # Extract predictions\n",
    "        combined_consumption = predictions[0]\n",
    "        highway_consumption = predictions[1]\n",
    "        emissions = predictions[2]\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        city_consumption = combined_consumption * 1.25  # Estimated city consumption\n",
    "        \n",
    "        # Efficiency rating\n",
    "        def get_efficiency_rating(consumption):\n",
    "            if consumption < 6: return \"Excellent\"\n",
    "            elif consumption < 8: return \"Good\"\n",
    "            elif consumption < 10: return \"Average\"\n",
    "            elif consumption < 12: return \"Below Average\"\n",
    "            else: return \"Poor\"\n",
    "        \n",
    "        # Annual cost estimation (assuming 15,000 km/year, $1.50/L)\n",
    "        annual_km = 15000\n",
    "        fuel_price = 1.50\n",
    "        annual_liters = (combined_consumption * annual_km) / 100\n",
    "        annual_cost = annual_liters * fuel_price\n",
    "        \n",
    "        # Environmental impact\n",
    "        annual_emissions_kg = (emissions * annual_km) / 1000  # Convert g to kg\n",
    "        \n",
    "        return {\n",
    "            'Predictions': {\n",
    "                'Combined_Consumption_L_100km': round(combined_consumption, 2),\n",
    "                'Highway_Consumption_L_100km': round(highway_consumption, 2),\n",
    "                'City_Consumption_L_100km': round(city_consumption, 2),\n",
    "                'CO2_Emissions_g_km': round(emissions, 2)\n",
    "            },\n",
    "            'Analysis': {\n",
    "                'Efficiency_Rating': get_efficiency_rating(combined_consumption),\n",
    "                'Annual_Fuel_Cost_CAD': round(annual_cost, 2),\n",
    "                'Annual_CO2_Emissions_kg': round(annual_emissions_kg, 2),\n",
    "                'Fuel_Efficiency_MPG': round(235.214583 / combined_consumption, 1)  # L/100km to MPG conversion\n",
    "            },\n",
    "            'Vehicle_Info': vehicle_specs\n",
    "        }\n",
    "    \n",
    "    return predict_fuel_consumption\n",
    "\n",
    "if 'best_model' in locals():\n",
    "    predict_fuel = create_prediction_function(best_model)\n",
    "    print(\"Enhanced prediction function created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prediction-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstration_predictions():\n",
    "    \"\"\"Demonstrate the prediction function with example vehicles.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PREDICTION DEMONSTRATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Example vehicles for demonstration\n",
    "    example_vehicles = [\n",
    "        {\n",
    "            'name': 'Compact Car',\n",
    "            'specs': {\n",
    "                'MAKE': 'TOYOTA',\n",
    "                'MODEL': 'COROLLA',\n",
    "                'VEHICLE CLASS': 'COMPACT',\n",
    "                'ENGINE SIZE': 1.8,\n",
    "                'CYLINDERS': 4,\n",
    "                'TRANSMISSION': 'AS',\n",
    "                'FUEL': 'X'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'Mid-size SUV',\n",
    "            'specs': {\n",
    "                'MAKE': 'BMW',\n",
    "                'MODEL': 'X5',\n",
    "                'VEHICLE CLASS': 'SUV: SMALL',\n",
    "                'ENGINE SIZE': 3.0,\n",
    "                'CYLINDERS': 6,\n",
    "                'TRANSMISSION': 'A8',\n",
    "                'FUEL': 'Z'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'Luxury Sedan',\n",
    "            'specs': {\n",
    "                'MAKE': 'MERCEDES-BENZ',\n",
    "                'MODEL': 'E-CLASS',\n",
    "                'VEHICLE CLASS': 'MID-SIZE',\n",
    "                'ENGINE SIZE': 2.0,\n",
    "                'CYLINDERS': 4,\n",
    "                'TRANSMISSION': 'A9',\n",
    "                'FUEL': 'Z'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'Pickup Truck',\n",
    "            'specs': {\n",
    "                'MAKE': 'FORD',\n",
    "                'MODEL': 'F-150',\n",
    "                'VEHICLE CLASS': 'PICKUP TRUCK: STANDARD',\n",
    "                'ENGINE SIZE': 5.0,\n",
    "                'CYLINDERS': 8,\n",
    "                'TRANSMISSION': 'A10',\n",
    "                'FUEL': 'Z'\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for vehicle in example_vehicles:\n",
    "        print(f\"\\n🚗 {vehicle['name']} Prediction:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            result = predict_fuel(vehicle['specs'])\n",
    "            \n",
    "            print(f\"Vehicle: {result['Vehicle_Info']['MAKE']} {result['Vehicle_Info']['MODEL']}\")\n",
    "            print(f\"Class: {result['Vehicle_Info']['VEHICLE CLASS']}\")\n",
    "            print(f\"Engine: {result['Vehicle_Info']['ENGINE SIZE']}L, {result['Vehicle_Info']['CYLINDERS']} cylinders\")\n",
    "            \n",
    "            print(\"\\nPredicted Consumption:\")\n",
    "            for key, value in result['Predictions'].items():\n",
    "                print(f\"  {key.replace('_', ' ')}: {value}\")\n",
    "            \n",
    "            print(\"\\nAnalysis:\")\n",
    "            for key, value in result['Analysis'].items():\n",
    "                print(f\"  {key.replace('_', ' ')}: {value}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error making prediction: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Demonstration completed!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "if 'predict_fuel' in locals():\n",
    "    demonstration_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 12. Summary and Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_summary():\n",
    "    \"\"\"Create a comprehensive summary of the analysis.\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"COMPREHENSIVE ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if 'df' in locals() and df is not None:\n",
    "        print(f\"\\n📊 Dataset Information:\")\n",
    "        print(f\"   • Total records: {len(df):,}\")\n",
    "        print(f\"   • Years covered: {df['YEAR'].min()} - {df['YEAR'].max()}\")\n",
    "        print(f\"   • Unique makes: {df['MAKE'].nunique()}\")\n",
    "        print(f\"   • Unique models: {df['MODEL'].nunique()}\")\n",
    "    \n",
    "    if 'best_model_name' in locals():\n",
    "        print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
    "        if 'all_metrics' in locals() and best_model_name in all_metrics:\n",
    "            metrics = all_metrics[best_model_name]['average']\n",
    "            print(f\"   • Average R² Score: {metrics['r2']:.4f}\")\n",
    "            print(f\"   • Average RMSE: {metrics['rmse']:.4f}\")\n",
    "            print(f\"   • Average MAE: {metrics['mae']:.4f}\")\n",
    "    \n",
    "    if 'model_path' in locals() and model_path:\n",
    "        print(f\"\\n💾 Model Saved:\")\n",
    "        print(f\"   • File: {os.path.basename(model_path)}\")\n",
    "        print(f\"   • Location: {os.path.dirname(model_path)}\")\n",
    "        if os.path.exists(model_path):\n",
    "            size_mb = os.path.getsize(model_path) / (1024 * 1024)\n",
    "            print(f\"   • Size: {size_mb:.2f} MB\")\n",
    "    \n",
    "    print(f\"\\n🎯 Model Capabilities:\")\n",
    "    print(f\"   • Predicts combined fuel consumption (L/100km)\")\n",
    "    print(f\"   • Predicts highway fuel consumption (L/100km)\")\n",
    "    print(f\"   • Predicts CO2 emissions (g/km)\")\n",
    "    print(f\"   • Provides efficiency ratings\")\n",
    "    print(f\"   • Estimates annual costs and environmental impact\")\n",
    "    \n",
    "    if 'importance_df' in locals() and importance_df is not None:\n",
    "        print(f\"\\n🔍 Key Insights:\")\n",
    "        top_feature = importance_df.iloc[0]['Feature']\n",
    "        print(f\"   • Most important feature: {top_feature}\")\n",
    "        if 'groups_df' in locals() and groups_df is not None:\n",
    "            top_group = groups_df.iloc[0]['Feature_Group']\n",
    "            print(f\"   • Most important feature group: {top_group}\")\n",
    "    \n",
    "    print(f\"\\n📈 Model Performance:\")\n",
    "    if 'comparison_df' in locals():\n",
    "        print(f\"   • Total models compared: {len(comparison_df)}\")\n",
    "        best_performance = comparison_df.iloc[0]\n",
    "        print(f\"   • Best model accuracy: {best_performance['Avg_R2']:.1%}\")\n",
    "        print(f\"   • Training time: {best_performance['Training_Time']:.2f} seconds\")\n",
    "    \n",
    "    print(f\"\\n🚀 Usage Instructions:\")\n",
    "    print(f\"   1. Load the model: model = joblib.load('{os.path.basename(model_path) if 'model_path' in locals() else 'model.pkl'}')\")\n",
    "    print(f\"   2. Use the prediction function with vehicle specifications\")\n",
    "    print(f\"   3. Required inputs: MAKE, MODEL, VEHICLE CLASS, ENGINE SIZE, CYLINDERS, TRANSMISSION, FUEL\")\n",
    "    print(f\"   4. Outputs: Fuel consumption predictions and comprehensive analysis\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"✅ ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"📁 All outputs saved to the model directory\")\n",
    "    print(\"🎉 Ready for deployment and real-world predictions!\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "create_final_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps-header",
   "metadata": {},
   "source": [
    "## 13. Next Steps and Usage Guide\n",
    "\n",
    "### Loading the Saved Model\n",
    "```python\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Load the model package\n",
    "model_package = joblib.load('model/best_fuel_model_[timestamp].pkl')\n",
    "model = model_package['model']\n",
    "input_features = model_package['input_features']\n",
    "output_features = model_package['output_features']\n",
    "```\n",
    "\n",
    "### Making Predictions\n",
    "```python\n",
    "# Example prediction\n",
    "vehicle_data = {\n",
    "    'MAKE': 'TOYOTA',\n",
    "    'MODEL': 'CAMRY',\n",
    "    'VEHICLE CLASS': 'MID-SIZE',\n",
    "    'ENGINE SIZE': 2.5,\n",
    "    'CYLINDERS': 4,\n",
    "    'TRANSMISSION': 'A8',\n",
    "    'FUEL': 'X'\n",
    "}\n",
    "\n",
    "input_df = pd.DataFrame([vehicle_data])\n",
    "predictions = model.predict(input_df)\n",
    "```\n",
    "\n",
    "### Model Performance\n",
    "The trained model provides highly accurate predictions for:\n",
    "- **Combined Fuel Consumption** (L/100 km)\n",
    "- **Highway Fuel Consumption** (L/100 km)  \n",
    "- **CO2 Emissions** (g/km)\n",
    "\n",
    "### Applications\n",
    "- Vehicle purchasing decisions\n",
    "- Fleet management optimization\n",
    "- Environmental impact assessment\n",
    "- Fuel cost budgeting\n",
    "- Regulatory compliance reporting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fuel_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
